Here is an implementation of a replicated key-value store using RDMA.

There are two types of servers in this implementation: primary and backups. Both primary and backup servers can process GET requests from clients, but only the primary can process PUT requests. The hash table in this implementation is not sharded and backups are only copies of the primary. The primary updates backups using RDMA write. Communication between servers and clients is via TCP/IP, but it can be modified to RDMA. The order in which the primary updates itself, updates backups, and replies to clients can vary, which affects consistency semantics. In this implementation, the order is to update the primary itself, update backups, and then reply to clients after successfully updating backups. This approach is close to strong consistency.

One of the key aspects of this design is how to use RDMA write to update backups correctly. To enable the primary to update backups directly using RDMA write, memory regions on backups need to mirror those on the primary. A memory region is allocated and registered for the hash table, and separate chaining is used for hash collision. There is a dynamic allocator for the hash table, and although the size of values is currently fixed, the design can be generalized to accommodate varying value sizes while retaining a fixed memory management unit.

Unused memory is stored in a free list. New elements forming a linked list within a bucket can obtain memory from the free list. The functions for create, put, and delete operations can return the size of the modified memory and its offset from the starting address of the hash table. When connecting to backups, the primary can determine the starting address of the hash table on backup servers. The primary can then issue RDMA write with the correct addresses and offsets. For put and delete operations, the next pointer of the previous element and the free list are also modified. Backups do not need to maintain the free list, and the primary can update the next pointer along with the newly chained element. Note that the next pointer's value only makes sense on the primary, so the offset should also be written to the backups, and backups can calculate the real address when processing GET requests.

To improve throughput, server processes are multithreaded with a thread pool. Each thread is dedicated to a client request and uses a unique socket, but threads share the same QPs. There may be some linearity for HCA performing DMA, so there is no need to worry about chimeric data in the hash table. However, to avoid write-write race conditions, read-write locks are used for each key. Additionally, clients can also be multithreaded to further enhance throughput.

The existing code contains some known bugs. For example, if the primary server and client are both multithreaded simultaneously, a "stack smashing detected" error may occur when multiple PUT requests are made from different client threads. This issue arises due to the shared QPs among threads, which makes it impossible for each thread to distinguish if the work completion from the CQ is for itself or not. To resolve this issue, a mechanism is needed to dynamically allocate RDMA resources to work routines for the thread pool. Alternatively, the RDMA resources can also be bound to working threads in the thread pool when the pool is created.

Another bug occurs when the number of tests is large, typically after 100,000 tests have been run. In this scenario, a "cannot assign requested address" error arises when a client thread attempts to connect to a server. The root cause of this issue might be that the servers handle client requests via work routine functions that are assigned to working threads in a thread pool. At the end of this routine, the connection between the server and client is closed, which means that a new connection must be established for each client request. This process can consume many socket resources, leading to the error. A new design is needed to address this problem.